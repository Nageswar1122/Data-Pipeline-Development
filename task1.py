# -*- coding: utf-8 -*-
"""task1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Et2Xwjh7mDOlPjcxyl8Y9yc0ohgdTueY
"""

import pandas as pd
import numpy as np
from sklearn.datasets import load_iris, fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from sklearn.ensemble import RandomForestClassifier
import warnings
warnings.filterwarnings('ignore')

class ETLPipeline:
    """
    A comprehensive ETL (Extract, Transform, Load) pipeline for data preprocessing
    using Pandas and Scikit-learn.
    """

    def __init__(self, scaling_method='standard'):
        """
        Initialize the ETL pipeline.

        Args:
            scaling_method (str): 'standard' for StandardScaler or 'minmax' for MinMaxScaler
        """
        self.scaling_method = scaling_method
        self.preprocessor = None
        self.pipeline = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None

    def extract_data(self, source='iris', file_path=None):
        """
        Extract data from various sources.

        Args:
            source (str): 'iris', 'housing', or 'csv' for custom CSV file
            file_path (str): Path to CSV file if source='csv'

        Returns:
            pd.DataFrame: The extracted dataset
        """
        print("üîÑ Step 1: Extracting data...")

        if source == 'iris':
            # Load Iris dataset
            data = load_iris()
            df = pd.DataFrame(data.data, columns=data.feature_names)
            df['target'] = data.target
            df['species'] = df['target'].map({0: 'setosa', 1: 'versicolor', 2: 'virginica'})

            # Add some missing values for demonstration
            np.random.seed(42)
            missing_indices = np.random.choice(df.index, size=15, replace=False)
            df.loc[missing_indices, 'sepal length (cm)'] = np.nan

            print(f"‚úÖ Loaded Iris dataset with {len(df)} rows and {len(df.columns)} columns")

        elif source == 'housing':
            # Load California Housing dataset
            data = fetch_california_housing()
            df = pd.DataFrame(data.data, columns=data.feature_names)
            df['target'] = data.target

            # Add categorical column and missing values for demonstration
            df['location_category'] = np.random.choice(['urban', 'suburban', 'rural'], size=len(df))
            np.random.seed(42)
            missing_indices = np.random.choice(df.index, size=200, replace=False)
            df.loc[missing_indices, 'AveRooms'] = np.nan

            print(f"‚úÖ Loaded California Housing dataset with {len(df)} rows and {len(df.columns)} columns")

        elif source == 'csv' and file_path:
            # Load custom CSV file
            df = pd.read_csv(file_path)
            print(f"‚úÖ Loaded custom dataset from {file_path} with {len(df)} rows and {len(df.columns)} columns")

        else:
            raise ValueError("Invalid source. Choose 'iris', 'housing', or 'csv' with file_path")

        # Display basic info about the dataset
        print(f"üìä Dataset shape: {df.shape}")
        print(f"üîç Missing values: {df.isnull().sum().sum()}")
        print(f"üìã Data types:\n{df.dtypes.value_counts()}")

        return df

    def transform_data(self, df, target_column=None):
        """
        Transform the data by handling missing values, encoding categorical variables,
        and preparing for feature scaling.

        Args:
            df (pd.DataFrame): Input dataframe
            target_column (str): Name of the target column

        Returns:
            tuple: (X, y) features and target
        """
        print("\nüîÑ Step 2: Transforming data...")

        # Identify target column if not specified
        if target_column is None:
            target_column = 'target'

        # Separate features and target
        if target_column in df.columns:
            X = df.drop(target_column, axis=1)
            y = df[target_column]
        else:
            X = df
            y = None

        # Identify column types
        numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()
        categorical_features = X.select_dtypes(include=['object']).columns.tolist()

        print(f"üìä Numeric features ({len(numeric_features)}): {numeric_features}")
        print(f"üìä Categorical features ({len(categorical_features)}): {categorical_features}")

        # Create preprocessing steps for numeric features
        numeric_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='median')),  # Handle missing values with median
            ('scaler', StandardScaler() if self.scaling_method == 'standard' else MinMaxScaler())
        ])

        # Create preprocessing steps for categorical features
        categorical_transformer = Pipeline(steps=[
            ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),  # Handle missing values
            ('onehot', OneHotEncoder(drop='first', sparse_output=False))  # One-hot encode
        ])

        # Combine preprocessing steps
        self.preprocessor = ColumnTransformer(
            transformers=[
                ('num', numeric_transformer, numeric_features),
                ('cat', categorical_transformer, categorical_features)
            ])

        print("‚úÖ Preprocessing pipeline created")
        return X, y

    def split_data(self, X, y, test_size=0.2, random_state=42):
        """
        Split the dataset into training and testing sets.

        Args:
            X (pd.DataFrame): Features
            y (pd.Series): Target variable
            test_size (float): Proportion of dataset for testing
            random_state (int): Random seed for reproducibility
        """
        print(f"\nüîÑ Step 3: Splitting data (test size: {test_size})...")

        if y is not None:
            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
                X, y, test_size=test_size, random_state=random_state, stratify=y if len(y.unique()) < 20 else None
            )
        else:
            self.X_train, self.X_test = train_test_split(
                X, test_size=test_size, random_state=random_state
            )
            self.y_train, self.y_test = None, None

        print(f"‚úÖ Training set: {self.X_train.shape[0]} samples")
        print(f"‚úÖ Testing set: {self.X_test.shape[0]} samples")

    def build_pipeline(self, model=None):
        """
        Build a complete ML pipeline with preprocessing and model.

        Args:
            model: Scikit-learn model (optional, defaults to RandomForestClassifier)
        """
        print("\nüîÑ Step 4: Building reusable pipeline...")

        if model is None:
            model = RandomForestClassifier(n_estimators=100, random_state=42)

        # Create complete pipeline
        self.pipeline = Pipeline(steps=[
            ('preprocessor', self.preprocessor),
            ('classifier', model)
        ])

        print("‚úÖ Complete ML pipeline created with preprocessing and model")
        return self.pipeline

    def fit_pipeline(self):
        """
        Fit the pipeline on training data.
        """
        if self.pipeline is None:
            raise ValueError("Pipeline not built. Call build_pipeline() first.")

        if self.y_train is None:
            raise ValueError("No target variable available for supervised learning.")

        print("\nüîÑ Step 5: Fitting pipeline on training data...")
        self.pipeline.fit(self.X_train, self.y_train)

        # Calculate training accuracy
        train_score = self.pipeline.score(self.X_train, self.y_train)
        test_score = self.pipeline.score(self.X_test, self.y_test)

        print(f"‚úÖ Pipeline fitted successfully!")
        print(f"üìä Training accuracy: {train_score:.4f}")
        print(f"üìä Testing accuracy: {test_score:.4f}")

    def transform_and_save(self, X, output_path, include_predictions=True):
        """
        Apply preprocessing transformations and save the cleaned dataset.

        Args:
            X (pd.DataFrame): Input features
            output_path (str): Path to save the transformed dataset
            include_predictions (bool): Whether to include model predictions
        """
        print(f"\nüîÑ Step 6: Transforming and saving data to {output_path}...")

        # Transform the data using the fitted preprocessor
        X_transformed = self.preprocessor.transform(X)

        # Get feature names after transformation
        feature_names = self.get_feature_names()

        # Create DataFrame with transformed features
        df_transformed = pd.DataFrame(X_transformed, columns=feature_names)

        # Add predictions if model is available and fitted
        if include_predictions and self.pipeline is not None and self.y_train is not None:
            try:
                predictions = self.pipeline.predict(X)
                df_transformed['predictions'] = predictions

                # Add prediction probabilities if available
                if hasattr(self.pipeline.named_steps['classifier'], 'predict_proba'):
                    proba = self.pipeline.predict_proba(X)
                    for i, class_name in enumerate(self.pipeline.classes_):
                        df_transformed[f'proba_class_{class_name}'] = proba[:, i]
            except Exception as e:
                print(f"‚ö†Ô∏è Could not add predictions: {e}")

        # Save to CSV
        df_transformed.to_csv(output_path, index=False)
        print(f"‚úÖ Transformed dataset saved with {len(df_transformed)} rows and {len(df_transformed.columns)} columns")

        return df_transformed

    def get_feature_names(self):
        """
        Get feature names after preprocessing transformation.

        Returns:
            list: List of feature names
        """
        feature_names = []

        # Get numeric feature names
        if hasattr(self.preprocessor.named_transformers_['num'], 'get_feature_names_out'):
            num_features = self.preprocessor.named_transformers_['num'].get_feature_names_out()
        else:
            num_features = [f'num__{name}' for name in self.preprocessor.transformers_[0][2]]
        feature_names.extend(num_features)

        # Get categorical feature names
        if 'cat' in [name for name, _, _ in self.preprocessor.transformers_]:
            if hasattr(self.preprocessor.named_transformers_['cat'], 'get_feature_names_out'):
                cat_features = self.preprocessor.named_transformers_['cat'].get_feature_names_out()
            else:
                cat_features = [f'cat__{name}' for name in self.preprocessor.transformers_[1][2]]
            feature_names.extend(cat_features)

        return feature_names

    def generate_report(self, df_original, df_transformed):
        """
        Generate a summary report of the ETL process.

        Args:
            df_original (pd.DataFrame): Original dataset
            df_transformed (pd.DataFrame): Transformed dataset
        """
        print("\n" + "="*60)
        print("üìã ETL PIPELINE SUMMARY REPORT")
        print("="*60)

        print(f"üìä Original dataset shape: {df_original.shape}")
        print(f"üìä Transformed dataset shape: {df_transformed.shape}")
        print(f"üîß Scaling method used: {self.scaling_method}")
        print(f"üìÇ Missing values in original: {df_original.isnull().sum().sum()}")
        print(f"üìÇ Missing values in transformed: {df_transformed.isnull().sum().sum()}")

        if self.pipeline is not None and self.y_train is not None:
            train_score = self.pipeline.score(self.X_train, self.y_train)
            test_score = self.pipeline.score(self.X_test, self.y_test)
            print(f"üéØ Model training accuracy: {train_score:.4f}")
            print(f"üéØ Model testing accuracy: {test_score:.4f}")

        print("‚úÖ ETL Pipeline completed successfully!")

def main():
    """
    Main function to demonstrate the complete ETL pipeline.
    """
    print("üöÄ Starting Complete ETL Data Pipeline Demo")
    print("="*60)

    # Initialize ETL pipeline
    etl = ETLPipeline(scaling_method='standard')

    # Step 1: Extract data (using Iris dataset)
    df = etl.extract_data(source='iris')

    # Display sample of original data
    print(f"\nüìã Sample of original data:")
    print(df.head())
    print(f"\nüìã Missing values per column:")
    print(df.isnull().sum())

    # Step 2: Transform data
    X, y = etl.transform_data(df, target_column='target')

    # Step 3: Split data
    etl.split_data(X, y, test_size=0.2)

    # Step 4: Build pipeline
    pipeline = etl.build_pipeline()

    # Step 5: Fit pipeline
    etl.fit_pipeline()

    # Step 6: Transform and save data
    df_transformed = etl.transform_and_save(
        etl.X_test,
        'transformed_dataset.csv',
        include_predictions=True
    )

    # Display sample of transformed data
    print(f"\nüìã Sample of transformed data:")
    print(df_transformed.head())

    # Generate summary report
    etl.generate_report(df, df_transformed)

    # Demonstrate with California Housing dataset
    print("\n" + "="*60)
    print("üè† BONUS: Running pipeline with California Housing dataset")
    print("="*60)

    etl_housing = ETLPipeline(scaling_method='minmax')
    df_housing = etl_housing.extract_data(source='housing')
    X_housing, y_housing = etl_housing.transform_data(df_housing, target_column='target')
    etl_housing.split_data(X_housing, y_housing)

    # Use a regression model for housing dataset
    from sklearn.ensemble import RandomForestRegressor
    pipeline_housing = etl_housing.build_pipeline(model=RandomForestRegressor(n_estimators=50, random_state=42))
    etl_housing.fit_pipeline()

    df_housing_transformed = etl_housing.transform_and_save(
        etl_housing.X_test,
        'transformed_housing_dataset.csv',
        include_predictions=True
    )

    print(f"\nüìã Housing dataset transformed and saved!")
    print(f"üìä Transformed housing data shape: {df_housing_transformed.shape}")

if __name__ == "__main__":
    main()

